<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tutorial Completo de Ollama</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 2em;
            color: #e0e0e0;
            background-color: #121212;
        }
        h1, h2 {
            color: #ffffff;
        }
        h1 {
            border-bottom: 2px solid #bb86fc;
            padding-bottom: 0.5em;
        }
        h2 {
            margin-top: 1.5em;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: #1e1e1e;
            padding: 2em;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
            border: 1px solid #333;
        }
        code, pre {
            background-color: #2d2d2d;
            padding: 2px 6px;
            border-radius: 4px;
            color: #f44336;
            font-family: "Courier New", Courier, monospace;
            border: 1px solid #444;
        }
        pre {
            padding: 1em;
            overflow-x: auto;
            white-space: pre-wrap;
        }
        .code-block {
            background-color: #1e1e1e;
            border: 1px solid #333;
            border-radius: 8px;
            padding: 1.5em;
            margin: 1em 0;
            font-family: "JetBrains Mono", "Fira Code", "Courier New", monospace;
            font-size: 14px;
            line-height: 1.5;
            color: #f8f8f2;
            overflow-x: auto;
            position: relative;
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }
        .code-block::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 30px;
            background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1);
            border-radius: 8px 8px 0 0;
            opacity: 0.8;
        }
        .code-block > br {
            display: block;
            content: "";
            margin-top: 0.5em;
        }
        /* Colores espec√≠ficos para bash/terminal Linux */
        .code-block .bash-comment {
            color: #6272a4;
            font-style: italic;
        }
        .code-block .bash-command {
            color: #50fa7b;
            font-weight: bold;
        }
        .code-block .bash-option {
            color: #ff6b6b;
        }
        .code-block .bash-path {
            color: #8be9fd;
        }
        .code-block .bash-string {
            color: #f1fa8c;
        }
        .code-block .bash-variable {
            color: #ffb86c;
        }
        .code-block .bash-number {
            color: #bd93f9;
        }
        .code-block .bash-symbol {
            color: #ff79c6;
        }
        .code-block .bash-user {
            color: #ff6b6b;
            font-weight: bold;
        }
        .code-block .bash-prompt {
            color: #6272a4;
        }
        .code-block .bash-error {
            color: #ff5555;
            font-weight: bold;
        }
        ul {
            list-style-type: disc;
            padding-left: 20px;
        }
        .card {
            background: #2a2a2a;
            border: 1px solid #444;
            border-radius: 6px;
            margin-bottom: 1.5em;
            padding: 1.5em;
        }
        .card-body {
            color: #e0e0e0;
        }
        footer {
            color: #b0b0b0;
            border-top: 1px solid #444;
        }
        a {
            color: #bb86fc;
            text-decoration: none;
        }
        a:hover {
            color: #9d4edd;
        }
        strong {
            color: #ffffff;
        }
        .intro {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        .warning {
            background: #fff3cd;
            color: #856404;
            padding: 15px;
            border: 1px solid #ffeaa7;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #f39c12;
        }
        .info {
            background: #d1ecf1;
            color: #0c5460;
            padding: 15px;
            border: 1px solid #b8daff;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #17a2b8;
        }
        .feature-list {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .feature-card {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #28a745;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
            transition: transform 0.2s;
            border: 1px solid #444;
        }
        .feature-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 16px rgba(0,0,0,0.1);
        }
        .step {
            background: #2a2a2a;
            padding: 20px;
            margin: 15px 0;
            border-radius: 8px;
            border-left: 4px solid #28a745;
            position: relative;
            border: 1px solid #444;
        }
        .step-number {
            position: absolute;
            left: -15px;
            top: -10px;
            background: #28a745;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 14px;
        }
        .command {
            background: #e8f5e8;
            color: #155724;
            padding: 8px 12px;
            border-radius: 5px;
            font-family: monospace;
            display: inline-block;
            margin: 5px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: #2a2a2a;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border: 1px solid #444;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #444;
            color: #e0e0e0;
        }
        th {
            background: #333;
            color: white;
            font-weight: 600;
        }
        .toc {
            background: #2a2a2a;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
            border: 1px solid #444;
        }
        .toc h3 {
            margin-top: 0;
            color: #ffffff;
        }
        .toc ul {
            margin-bottom: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü¶ô Tutorial Completo de Ollama</h1>
        
        <div class="intro">
            <h2 style="border: none; padding: 0; margin: 0 0 15px 0;">¬øQu√© es Ollama?</h2>
            <p>Ollama es una herramienta que te permite ejecutar modelos de lenguaje grandes (LLMs) localmente en tu m√°quina de forma sencilla y eficiente. Piensa en ello como tu propio ChatGPT personal que funciona sin internet y mantiene toda tu informaci√≥n privada.</p>
        </div>

        <div class="toc">
            <h3>üìã Tabla de Contenidos</h3>
            <ul>
                <li><a href="#que-es-ollama">¬øQu√© es Ollama?</a></li>
                <li><a href="#caracteristicas">Caracter√≠sticas Principales</a></li>
                <li><a href="#instalacion-linux">Instalaci√≥n en Linux</a></li>
                <li><a href="#primeros-pasos">Primeros Pasos</a></li>
                <li><a href="#modificacion-modelos">Modificaci√≥n de Modelos</a></li>
                <li><a href="#integracion">Integraci√≥n con Aplicaciones</a></li>
                <li><a href="#comandos">Comandos √ötiles</a></li>
                <li><a href="#troubleshooting">Resoluci√≥n de Problemas</a></li>
            </ul>
        </div>

        <section id="que-es-ollama">
            <h2>ü§î ¬øQu√© es Ollama?</h2>
            <p>Ollama es una aplicaci√≥n de c√≥digo abierto que simplifica la ejecuci√≥n de modelos de lenguaje grandes en tu computadora local. Desarrollada para ser f√°cil de usar, Ollama abstrae la complejidad t√©cnica que normalmente conlleva ejecutar estos modelos.</p>
            
            <h3>¬øPor qu√© usar Ollama?</h3>
            <ul>
                <li><strong>Privacidad Total:</strong> Tus datos nunca salen de tu computadora</li>
                <li><strong>Sin Conexi√≥n a Internet:</strong> Funciona completamente offline</li>
                <li><strong>Gratuito:</strong> No hay costos por uso o tokens</li>
                <li><strong>Personalizable:</strong> Puedes modificar y ajustar los modelos seg√∫n tus necesidades</li>
                <li><strong>M√∫ltiples Modelos:</strong> Soporte para cientos de modelos diferentes</li>
            </ul>
        </section>

        <section id="caracteristicas">
            <h2>‚ú® Caracter√≠sticas Principales</h2>
            
            <div class="feature-list">
                <div class="feature-card">
                    <h4>üöÄ F√°cil Instalaci√≥n</h4>
                    <p>Instalaci√≥n con un solo comando en m√∫ltiples sistemas operativos.</p>
                </div>
                <div class="feature-card">
                    <h4>üì¶ Gesti√≥n de Modelos</h4>
                    <p>Descarga, actualiza y gestiona modelos con comandos simples.</p>
                </div>
                <div class="feature-card">
                    <h4>üîß API REST</h4>
                    <p>API completa para integrar con tus aplicaciones.</p>
                </div>
                <div class="feature-card">
                    <h4>üíª Multiplataforma</h4>
                    <p>Funciona en Linux, macOS y Windows.</p>
                </div>
                <div class="feature-card">
                    <h4>üéØ Optimizaci√≥n</h4>
                    <p>Optimizado para diferentes tipos de hardware.</p>
                </div>
                <div class="feature-card">
                    <h4>üîÑ Streaming</h4>
                    <p>Respuestas en tiempo real con streaming de tokens.</p>
                </div>
            </div>
        </section>

        <section id="instalacion-linux">
            <h2>üêß Instalaci√≥n en Linux</h2>
            
            <div class="step">
                <div class="step-number">1</div>
                <h3>Instalaci√≥n Autom√°tica</h3>
                <p>La forma m√°s sencilla de instalar Ollama en Linux es usando el script oficial:</p>
                <div class="code-block"><span class="bash-command">curl</span> <span class="bash-option">-fsSL</span> <span class="bash-string">https://ollama.com/install.sh</span> <span class="bash-symbol">|</span> <span class="bash-command">sh</span></div>
                <div class="info">
                    <strong>Nota:</strong> Este script detectar√° autom√°ticamente tu distribuci√≥n de Linux e instalar√° Ollama adecuadamente.
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <h3>Instalaci√≥n Manual (Ubuntu/Debian)</h3>
                <p>Si prefieres una instalaci√≥n manual:</p>
                <div class="code-block">
<span class="bash-comment"># Descargar el paquete</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">curl</span> <span class="bash-option">-L</span> <span class="bash-string">https://ollama.com/download/ollama-linux-amd64.tgz</span> <span class="bash-option">-o</span> <span class="bash-path">/tmp/ollama.tgz</span><br>
<br>
<span class="bash-comment"># Extraer a /usr/local/bin</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">tar</span> <span class="bash-option">-C</span> <span class="bash-path">/usr/local/bin</span> <span class="bash-option">-xzf</span> <span class="bash-path">/tmp/ollama.tgz</span><br>
<br>
<span class="bash-comment"></span># Hacer ejecutable</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">chmod</span> <span class="bash-option">+x</span> <span class="bash-path">/usr/local/bin/ollama</span><br>
                </div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <h3>Crear Servicio del Sistema</h3>
                <p>Para que Ollama se ejecute como servicio:</p>
                <div class="code-block">
<span class="bash-comment"># Crear usuario para ollama</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">useradd</span> <span class="bash-option">-r</span> <span class="bash-option">-s</span> <span class="bash-path">/bin/false</span> <span class="bash-option">-m</span> <span class="bash-option">-d</span> <span class="bash-path">/usr/share/ollama</span> <span class="bash-variable">ollama</span><br>
<br>
<span class="bash-comment"># Crear archivo de servicio</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">tee</span> <span class="bash-path">/etc/systemd/system/ollama.service</span> <span class="bash-symbol"><<</span><span class="bash-variable">EOF</span><br>
<span class="bash-symbol">[</span><span class="bash-variable">Unit</span><span class="bash-symbol">]</span><br>
<span class="bash-variable">Description</span><span class="bash-symbol">=</span><span class="bash-string">Ollama Service</span><br>
<span class="bash-variable">After</span><span class="bash-symbol">=</span><span class="bash-string">network-online.target</span><br>
<br>
<span class="bash-symbol">[</span><span class="bash-variable">Service</span><span class="bash-symbol">]</span><br>
<span class="bash-variable">ExecStart</span><span class="bash-symbol">=</span><span class="bash-path">/usr/local/bin/ollama serve</span><br>
<span class="bash-variable">User</span><span class="bash-symbol">=</span><span class="bash-variable">ollama</span><br>
<span class="bash-variable">Group</span><span class="bash-symbol">=</span><span class="bash-variable">ollama</span><br>
<span class="bash-variable">Restart</span><span class="bash-symbol">=</span><span class="bash-string">always</span><br>
<span class="bash-variable">RestartSec</span><span class="bash-symbol">=</span><span class="bash-number">3</span><br>
<span class="bash-variable">Environment</span><span class="bash-symbol">=</span><span class="bash-string">"HOME=/usr/share/ollama"</span><br>
<span class="bash-variable">Environment</span><span class="bash-symbol">=</span><span class="bash-string">"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"</span><br>
<br>
<span class="bash-symbol">[</span><span class="bash-variable">Install</span><span class="bash-symbol">]</span><br>
<span class="bash-variable">WantedBy</span><span class="bash-symbol">=</span><span class="bash-string">default.target</span><br>
<span class="bash-variable">EOF</span><br>
<br>
<span class="bash-comment"># Recargar y habilitar el servicio</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">systemctl</span> <span class="bash-command">daemon-reload</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">systemctl</span> <span class="bash-command">enable</span> <span class="bash-variable">ollama</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">systemctl</span> <span class="bash-command">start</span> <span class="bash-variable">ollama</span><br>
                </div>
            </div>

            <div class="step">
                <div class="step-number">4</div>
                <h3>Verificar Instalaci√≥n</h3>
                <p>Comprueba que Ollama est√° funcionando correctamente:</p>
                <div class="code-block">
<span class="bash-comment"></span># Verificar estado del servicio</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">systemctl</span> <span class="bash-command">status</span> <span class="bash-variable">ollama</span><br>
<br>
<span class="bash-comment"># Verificar que responde</span><br>
<span class="bash-command">curl</span> <span class="bash-string">http://localhost:11434/api/tags</span><br>
                </div>
            </div>
        </section>

        <section id="primeros-pasos">
            <h2>üöÄ Primeros Pasos</h2>

            <div class="step">
                <div class="step-number">1</div>
                <h3>Descargar tu Primer Modelo</h3>
                <p>Comencemos descargando un modelo popular como Llama 3.2:</p>
                <div class="code-block"><span class="bash-command">ollama</span> <span class="bash-command">pull</span> <span class="bash-variable">llama3.2</span></div>
                <div class="info">
                    <strong>Tip:</strong> Este comando descargar√° autom√°ticamente la versi√≥n m√°s peque√±a (3B) si no especificas el tama√±o.
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <h3>Ejecutar el Modelo</h3>
                <p>Una vez descargado, puedes chatear con el modelo:</p>
                <div class="code-block"><span class="bash-command">ollama</span> <span class="bash-command">run</span> <span class="bash-variable">llama3.2</span></div>
                <p>Esto abrir√° una sesi√≥n de chat interactiva. ¬°Prueba a hacerle una pregunta!</p>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <h3>Listar Modelos Disponibles</h3>
                <p>Para ver qu√© modelos tienes instalados:</p>
                <div class="code-block"><span class="bash-command">ollama</span> <span class="bash-command">list</span></div>
            </div>

            <h3>Modelos Populares para Comenzar</h3>
            <table>
                <thead>
                    <tr>
                        <th>Modelo</th>
                        <th>Tama√±o</th>
                        <th>RAM Requerida</th>
                        <th>Descripci√≥n</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>llama3.2:3b</td>
                        <td>2GB</td>
                        <td>4GB</td>
                        <td>Modelo peque√±o y r√°pido para tareas generales</td>
                    </tr>
                    <tr>
                        <td>llama3.2:7b</td>
                        <td>4GB</td>
                        <td>8GB</td>
                        <td>Buen equilibrio entre rendimiento y recursos</td>
                    </tr>
                    <tr>
                        <td>codellama:7b</td>
                        <td>4GB</td>
                        <td>8GB</td>
                        <td>Especializado en programaci√≥n</td>
                    </tr>
                    <tr>
                        <td>deepseek-r1:14b</td>
                        <td>8GB</td>
                        <td>16GB</td>
                        <td>Excelente para razonamiento complejo</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="modificacion-modelos">
            <h2>üîß Modificaci√≥n de Modelos</h2>
            <p>Una de las caracter√≠sticas m√°s potentes de Ollama es la capacidad de modificar y personalizar modelos existentes.</p>

            <div class="step">
                <div class="step-number">1</div>
                <h3>Modificar Ventana de Contexto</h3>
                <p>Por defecto, Ollama usa una ventana de contexto de 2048 tokens, que suele ser insuficiente para tareas complejas de programaci√≥n. Puedes aumentarla:</p>
                <div class="code-block">
<span class="bash-comment"># Ejecutar el modelo</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">run</span> <span class="bash-variable">deepseek-r1:14b</span><br>
<br>
<span class="bash-comment"># Dentro del chat, cambiar el par√°metro</span><br>
<span class="bash-string">/set parameter num_ctx 8192</span><br>
<br>
<span class="bash-comment"># Guardar el modelo modificado</span><br>
<span class="bash-string">/save deepseek-r1:14b-8k</span><br>
                </div>
                <div class="warning">
                    <strong>Importante:</strong> Aumentar la ventana de contexto requiere m√°s RAM. Una ventana de 8192 tokens puede necesitar 2-3GB adicionales de memoria.
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <h3>Crear Modelos Personalizados con Modelfile</h3>
                <p>Puedes crear variaciones personalizadas usando un archivo Modelfile:</p>
                <div class="code-block">
<span class="bash-comment"># Crear un archivo llamado Modelfile</span><br>
<span class="bash-variable">FROM</span> <span class="bash-variable">llama3.2:7b</span><br>
<br>
<span class="bash-comment"># Establecer par√°metros</span><br>
<span class="bash-variable">PARAMETER</span> <span class="bash-variable">temperature</span> <span class="bash-number">0.7</span><br>
<span class="bash-variable">PARAMETER</span> <span class="bash-variable">num_ctx</span> <span class="bash-number">4096</span><br>
<span class="bash-variable">PARAMETER</span> <span class="bash-variable">top_k</span> <span class="bash-number">40</span><br>
<span class="bash-variable">PARAMETER</span> <span class="bash-variable">top_p</span> <span class="bash-number">0.9</span><br>
<br>
<span class="bash-comment"># Establecer el prompt del sistema</span><br>
<span class="bash-variable">SYSTEM</span> <span class="bash-string">"""<br>
Eres un asistente de programaci√≥n experto.<br>
Siempre proporcionas c√≥digo limpio, bien comentado y siguiendo las mejores pr√°cticas.<br>
"""</span><br>
                </div>
                <p>Luego crear el modelo personalizado:</p>
                <div class="code-block"><span class="bash-command">ollama</span> <span class="bash-command">create</span> <span class="bash-variable">mi-programador</span> <span class="bash-option">-f</span> <span class="bash-path">./Modelfile</span></div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <h3>Par√°metros Comunes para Modificar</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Par√°metro</th>
                            <th>Descripci√≥n</th>
                            <th>Rango T√≠pico</th>
                            <th>Efecto</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>temperature</td>
                            <td>Creatividad de las respuestas</td>
                            <td>0.1 - 1.0</td>
                            <td>M√°s alto = m√°s creativo</td>
                        </tr>
                        <tr>
                            <td>num_ctx</td>
                            <td>Ventana de contexto (tokens)</td>
                            <td>1024 - 32768</td>
                            <td>M√°s alto = m√°s memoria de conversaci√≥n</td>
                        </tr>
                        <tr>
                            <td>top_k</td>
                            <td>N√∫mero de tokens candidatos</td>
                            <td>10 - 100</td>
                            <td>M√°s alto = m√°s variedad</td>
                        </tr>
                        <tr>
                            <td>top_p</td>
                            <td>Probabilidad acumulativa</td>
                            <td>0.1 - 1.0</td>
                            <td>M√°s alto = m√°s diversidad</td>
                        </tr>
                        <tr>
                            <td>repeat_penalty</td>
                            <td>Penalizaci√≥n por repetici√≥n</td>
                            <td>1.0 - 1.3</td>
                            <td>M√°s alto = menos repetitivo</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="step">
                <div class="step-number">4</div>
            <h3>Comandos de Chat √ötiles</h3>
            <p>Cuando est√©s chateando con un modelo, puedes usar estos comandos:</p>
            <div class="code-block">
<span class="bash-string">/set parameter num_ctx 8192</span>    <span class="bash-comment"># Cambiar ventana de contexto</span><br>
<span class="bash-string">/set parameter temperature 0.3</span>  <span class="bash-comment"># Cambiar creatividad</span><br>
<span class="bash-string">/save mi-modelo-modificado</span>      <span class="bash-comment"># Guardar configuraci√≥n actual</span><br>
<span class="bash-string">/show</span>                          <span class="bash-comment"># Mostrar informaci√≥n del modelo</span><br>
<span class="bash-string">/load</span>                          <span class="bash-comment"># Cargar imagen (modelos multimodales)</span><br>
<span class="bash-string">/bye</span>                          <span class="bash-comment"># Salir del chat</span><br>
            </div>
            </div>
        </section>

        <section id="integracion">
            <h2>üîó Integraci√≥n con Aplicaciones</h2>
            <p>Ollama proporciona una API REST que permite integrarlo f√°cilmente con otras aplicaciones.</p>

            <div class="step">
                <div class="step-number">1</div>
                <h3>Configurar el Servidor</h3>
                <p>Primero, aseg√∫rate de que el servidor est√© ejecut√°ndose:</p>
                <div class="code-block"><span class="bash-command">ollama</span> <span class="bash-command">serve</span></div>
                <p>Por defecto, el servidor se ejecuta en <span class="command">http://localhost:11434</span></p>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <h3>Ejemplo de Integraci√≥n con 16x Prompt</h3>
                <p>Basado en la gu√≠a proporcionada, aqu√≠ est√° el proceso para integrar con 16x Prompt:</p>
                <div class="code-block">
<span class="bash-comment"># 1. Descargar y configurar un modelo con mayor contexto</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">pull</span> <span class="bash-variable">deepseek-r1:14b</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">run</span> <span class="bash-variable">deepseek-r1:14b</span><br>
<span class="bash-string">/set parameter num_ctx 8192</span><br>
<span class="bash-string"></span>/save deepseek-r1:14b-8k</span><br>
<br>
<span class="bash-comment"># 2. Servir el modelo</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">serve</span><br>
                </div>
                <p>En 16x Prompt, configura:</p>
                <ul>
                    <li><strong>API Endpoint:</strong> <span class="command">http://127.0.0.1:11434/v1</span></li>
                    <li><strong>Model ID:</strong> <span class="command">deepseek-r1:14b-8k</span></li>
                    <li><strong>API Type:</strong> <span class="command">ollama</span></li>
                </ul>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <h3>Ejemplo de API con curl</h3>
                <div class="code-block">
<span class="bash-comment"># Chat completion</span><br>
<span class="bash-command">curl</span> <span class="bash-string">http://localhost:11434/api/chat</span> <span class="bash-option">-d</span> <span class="bash-string">'{
  "model": "llama3.2",
  "messages": [
    {"role": "user", "content": "¬øPor qu√© el cielo es azul?"}
  ]
}'</span><br>
<br>
<span class="bash-comment"># Generaci√≥n simple</span><br>
<span class="bash-command">curl</span> <span class="bash-string">http://localhost:11434/api/generate</span> <span class="bash-option">-d</span> <span class="bash-string">'{
  "model": "llama3.2",
  "prompt": "Explica la fotos√≠ntesis en t√©rminos simples"
}'</span><br>
                </div>
            </div>
        </section>

        <section id="comandos">
            <h2>‚å®Ô∏è Comandos √ötiles</h2>

            <h3>Gesti√≥n de Modelos</h3>
            <div class="code-block">
<span class="bash-comment"># Listar modelos instalados</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">list</span><br>
<br>
<span class="bash-comment"># Descargar un modelo</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">pull</span> <span class="bash-variable">model_name</span><br>
<br>
<span class="bash-comment"></span># Eliminar un modelo</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">rm</span> <span class="bash-variable">model_name</span><br>
<br>
<span class="bash-comment"># Mostrar informaci√≥n detallada</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">show</span> <span class="bash-variable">model_name</span><br>
<br>
# Copiar un modelo<br>
ollama cp source_model new_model<br>
            </div>

            <h3>Ejecuci√≥n y Chat</h3>
            <div class="code-block">
<span class="bash-comment"># Ejecutar modelo interactivamente</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">run</span> <span class="bash-variable">model_name</span><br>
<br>
<span class="bash-comment"># Ejecutar con prompt directo</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">run</span> <span class="bash-variable">model_name</span> <span class="bash-string">"Tu pregunta aqu√≠"</span><br>
<br>
<span class="bash-comment"># Servir la API</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">serve</span><br>
<br>
<span class="bash-comment"># Servir en puerto espec√≠fico</span><br>
<span class="bash-variable">OLLAMA_HOST</span><span class="bash-symbol">=</span><span class="bash-number">0.0.0.0:8080</span> <span class="bash-command">ollama</span> <span class="bash-command">serve</span><br>
            </div>

            <h3>Variables de Entorno √ötiles</h3>
            <table>
                <thead>
                    <tr>
                        <th>Variable</th>
                        <th>Descripci√≥n</th>
                        <th>Ejemplo</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>OLLAMA_HOST</td>
                        <td>Direcci√≥n y puerto del servidor</td>
                        <td>0.0.0.0:11434</td>
                    </tr>
                    <tr>
                        <td>OLLAMA_MODELS</td>
                        <td>Directorio de modelos</td>
                        <td>/custom/models/path</td>
                    </tr>
                    <tr>
                        <td>OLLAMA_NUM_PARALLEL</td>
                        <td>Requests paralelos m√°ximos</td>
                        <td>4</td>
                    </tr>
                    <tr>
                        <td>OLLAMA_MAX_LOADED_MODELS</td>
                        <td>Modelos cargados simult√°neamente</td>
                        <td>3</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="troubleshooting">
            <h2>üîç Resoluci√≥n de Problemas</h2>

            <h3>Problemas Comunes y Soluciones</h3>

            <div class="warning">
                <h4>Error: "Out of Memory"</h4>
                <p><strong>Causa:</strong> El modelo es demasiado grande para tu RAM disponible.</p>
                <p><strong>Soluci√≥n:</strong></p>
                <ul>
                    <li>Usa un modelo m√°s peque√±o (ej: 3B en lugar de 7B)</li>
                    <li>Cierra otras aplicaciones que consuman memoria</li>
                    <li>Reduce la ventana de contexto: <span class="command">/set parameter num_ctx 1024</span></li>
                </ul>
            </div>

            <div class="warning">
                <h4>Error: "Connection Refused"</h4>
                <p><strong>Causa:</strong> El servidor Ollama no est√° ejecut√°ndose.</p>
                <p><strong>Soluci√≥n:</strong></p>
                <div class="code-block">
<span class="bash-comment"></span># Iniciar el servidor</span><br>
<span class="bash-command">ollama</span> <span class="bash-command">serve</span><br>
<br>
<span class="bash-comment"># O si tienes el servicio del sistema</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">systemctl</span> <span class="bash-command">start</span> <span class="bash-variable">ollama</span><br>
                </div>
            </div>

            <div class="warning">
                <h4>Respuestas Muy Lentas</h4>
                <p><strong>Causas y soluciones:</strong></p>
                <ul>
                    <li><strong>CPU lenta:</strong> Considera usar modelos m√°s peque√±os</li>
                    <li><strong>Falta de GPU:</strong> Ollama puede usar GPU para acelerar</li>
                    <li><strong>Ventana de contexto grande:</strong> Reduce num_ctx</li>
                    <li><strong>M√∫ltiples modelos:</strong> Descarga solo los que uses</li>
                </ul>
            </div>

            <h3>Verificaci√≥n del Sistema</h3>
            <div class="code-block">
<span class="bash-comment"># Ver logs del sistema</span><br>
<span class="bash-command">sudo</span> <span class="bash-command">journalctl</span> <span class="bash-option">-u</span> <span class="bash-variable">ollama</span> <span class="bash-option">-f</span><br>
<br>
<span class="bash-comment"># Ver procesos de Ollama</span><br>
<span class="bash-command">ps</span> <span class="bash-command">aux</span> <span class="bash-symbol">|</span> <span class="bash-command">grep</span> <span class="bash-variable">ollama</span><br>
<br>
<span class="bash-comment"># Verificar uso de memoria</span><br>
<span class="bash-command">free</span> <span class="bash-option">-h</span><br>
<br>
<span class="bash-comment"># Verificar conexi√≥n API</span><br>
<span class="bash-command">curl</span> <span class="bash-string">http://localhost:11434/api/tags</span><br>
            </div>

            <h3>Optimizaci√≥n del Rendimiento</h3>
            <div class="info">
                <h4>Consejos para Mejor Rendimiento</h4>
                <ul>
                    <li><strong>GPU:</strong> Si tienes NVIDIA GPU, aseg√∫rate de tener CUDA instalado</li>
                    <li><strong>RAM:</strong> M√°s RAM permite modelos m√°s grandes y mejor rendimiento</li>
                    <li><strong>SSD:</strong> Instala modelos en SSD para carga m√°s r√°pida</li>
                    <li><strong>Ventana de contexto:</strong> Usa solo el tama√±o que necesites</li>
                    <li><strong>Modelos quantizados:</strong> Los modelos Q4 y Q5 son m√°s eficientes</li>
                </ul>
            </div>
        </section>

        <h2>üéØ Conclusi√≥n</h2>
        <p>Ollama es una herramienta poderosa que democratiza el acceso a los modelos de lenguaje grandes. Con este tutorial, tienes todo lo necesario para:</p>
        <ul>
            <li>Instalar y configurar Ollama en Linux</li>
            <li>Descargar y ejecutar diferentes modelos</li>
            <li>Personalizar modelos seg√∫n tus necesidades</li>
            <li>Integrar Ollama con otras aplicaciones</li>
            <li>Resolver problemas comunes</li>
        </ul>
        
        <div class="info">
            <p><strong>Pr√≥ximos Pasos:</strong> Experimenta con diferentes modelos, prueba distintas configuraciones de par√°metros, y explora las posibilidades de integraci√≥n con tus herramientas favoritas.</p>
        </div>

        <p style="text-align: center; margin-top: 40px; color: #6c757d; font-style: italic;">
            ¬°Disfruta explorando el mundo de la IA local con Ollama! üöÄ
        </p>

        <footer style="text-align: center; margin-top: 2em; padding-top: 1em; border-top: 1px solid #444;">
            <p><a href="./pildoras.html">Volver a la p√°gina principal de p√≠ldoras</a></p>
        </footer>
    </div>
</body>
</html>
